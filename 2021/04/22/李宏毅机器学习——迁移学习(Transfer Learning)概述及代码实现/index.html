<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="基于PaddlePaddle的李宏毅机器学习——迁移学习 大噶好，我是healerson，一名入门不久自学AI的AI Trainer,宝可梦训练师（纯属业余）。希望能和大家共进步，错误之处恳请指出！百度AI Studio个人主页, 我在AI Studio上获得白银等级，点亮2个徽章，来互关呀~   本项目是在飞桨深度学习学院提供的李宏毅-机器学习特训营课程。 Abstract本文共分为两大部分：第">
<meta property="og:type" content="article">
<meta property="og:title" content="the world of healerson">
<meta property="og:url" content="http://example.com/2021/04/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0(Transfer%20Learning)%E6%A6%82%E8%BF%B0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/index.html">
<meta property="og:site_name" content="the world of healerson">
<meta property="og:description" content="基于PaddlePaddle的李宏毅机器学习——迁移学习 大噶好，我是healerson，一名入门不久自学AI的AI Trainer,宝可梦训练师（纯属业余）。希望能和大家共进步，错误之处恳请指出！百度AI Studio个人主页, 我在AI Studio上获得白银等级，点亮2个徽章，来互关呀~   本项目是在飞桨深度学习学院提供的李宏毅-机器学习特训营课程。 Abstract本文共分为两大部分：第">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415122437342.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415122631945.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415122751953.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415123031818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415123211214.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2021041512335336.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415123842675.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210416100235181.gif#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415124649256.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415124708437.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415124809586.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415125108449.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415140556375.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415141052765.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415141833915.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415141414902.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/a0fb7213eb449144f30d5dc5639b2885.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415142100358.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415142139276.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/a93819091f5dd5f245495f4e342caba4.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/69eeea1f5f0fb8b61a8b05200982a564.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415142358336.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415142857386.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415142909487.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/c45384f70bd12de95861b9fdfcc724b0.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/8c577e960c5a01067c1a90109ef61dd0.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/8f4a8c8d55946784b2c7bb986017130c.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415143820728.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2021041514385115.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415144213282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415144533297.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415144635963.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415144820434.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/69eeea1f5f0fb8b61a8b05200982a564.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/370c88f7f57d4b7b564a36cab7aaba3b.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415145842614.jpg?ype_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415145854814.jpg?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415145906343.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045964.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045975.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045971.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045967.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045956.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045956.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045968.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045959.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045951.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210415150045959.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/428bdfb77336911ac1f5df797d4e4e22.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/4a378a5f4dbe078ec6161382c5d6841c.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/24f7bf78e34fd7b13b401e3db9be523e.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/ec20633f7d6764a70a9ebb672cb9fca1.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/1293fe5e2ec18495617bef612711df9d.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/abbc156df82dbd3d44fdf5f2befd49f1.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210416095102961.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210416095113511.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210416095123218.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/2a96b01eda15d1f79ac63fec6d2faf9b.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/305bd558dfbbb20033ac809253990c80.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/f438de59acd27a79c2da6e6f1b0f5562.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/942116228473e0d6b4e03edfc87431b8.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210416095346634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2021041609540751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2021-04-22T12:53:19.179Z">
<meta property="article:modified_time" content="2021-04-22T12:54:38.531Z">
<meta property="article:author" content="healerson">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20210415122437342.png">

<link rel="canonical" href="http://example.com/2021/04/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0(Transfer%20Learning)%E6%A6%82%E8%BF%B0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title> | the world of healerson</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">the world of healerson</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0(Transfer%20Learning)%E6%A6%82%E8%BF%B0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="healerson">
      <meta itemprop="description" content="choosing is more important">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the world of healerson">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-04-22 20:53:19 / 修改时间：20:54:38" itemprop="dateCreated datePublished" datetime="2021-04-22T20:53:19+08:00">2021-04-22</time>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="基于PaddlePaddle的李宏毅机器学习——迁移学习"><a href="#基于PaddlePaddle的李宏毅机器学习——迁移学习" class="headerlink" title="基于PaddlePaddle的李宏毅机器学习——迁移学习"></a>基于PaddlePaddle的李宏毅机器学习——迁移学习</h1><blockquote>
<p>大噶好，我是healerson，一名入门不久自学AI的AI Trainer,宝可梦训练师（纯属业余）。希望能和大家共进步，错误之处恳请指出！<br><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/personalcenter/thirdview/616597">百度AI Studio个人主页</a>, 我在AI Studio上获得白银等级，点亮2个徽章，来互关呀~</p>
</blockquote>
<blockquote>
<h3 id="本项目是在飞桨深度学习学院提供的李宏毅-机器学习特训营课程。"><a href="#本项目是在飞桨深度学习学院提供的李宏毅-机器学习特训营课程。" class="headerlink" title="本项目是在飞桨深度学习学院提供的李宏毅-机器学习特训营课程。"></a>本项目是在飞桨深度学习学院提供的<a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/education/group/info/1978">李宏毅-机器学习</a>特训营课程。</h3></blockquote>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文共分为两大部分：第一部分介绍迁移学习的主要概念以及类型，第二部分是实现迁移学习布置的作业——领域对抗性训练(Domain Adversarial Training)并进行了三次不同epoch的训练。</p>
<h1 id="第一部分：迁移学习介绍"><a href="#第一部分：迁移学习介绍" class="headerlink" title="第一部分：迁移学习介绍"></a>第一部分：迁移学习介绍</h1><h2 id="1-迁移学习：Transfer-Learning"><a href="#1-迁移学习：Transfer-Learning" class="headerlink" title="1 迁移学习：Transfer Learning"></a>1 迁移学习：Transfer Learning</h2><h2 id="1-1-什么是迁移学习呢？"><a href="#1-1-什么是迁移学习呢？" class="headerlink" title="1.1 什么是迁移学习呢？"></a>1.1 什么是迁移学习呢？</h2><p>假设现在要做猫和狗的分类器，我们需要一样标签数据告诉机器哪些是猫，哪些是狗。<br>同时，假设现在有一些与猫和狗没有直接关系的数据，这里说是没有直接关系，并不是说是完全没有关系。就是说有一些关系，但又不是直接相关的。<br><img src="https://img-blog.csdnimg.cn/20210415122437342.png" alt="在这里插入图片描述"><br>假设现在有自然界真实存在的老虎和大象的图片，那老虎和大象对分辨猫和狗会有帮助吗。<br><img src="https://img-blog.csdnimg.cn/20210415122631945.png" alt="在这里插入图片描述"><br>或者说我们有一些卡通动画中的猫和狗图像，但不是真实存在的，有没有帮助呢。<br><img src="https://img-blog.csdnimg.cn/20210415122751953.png" alt="在这里插入图片描述"><br>迁移学习把任务A开发的模型作为初始点，重新使用在为任务B开发模型的过程中。迁移学习是通过从已学习的相关任务中转移知识来改进学习的新任务。</p>
<h2 id="1-2-为什么用迁移学习"><a href="#1-2-为什么用迁移学习" class="headerlink" title="1.2 为什么用迁移学习"></a>1.2 为什么用迁移学习</h2><p>这三个说的是，第一个是做闽南语(台湾腔)的语音识别，但是没有太多的训练数据，只有很多无直接关系的英文、普通话数据；第二是做医疗方面的图像识别，同样样本不多，但有很多其他真实动物的图像；第三个说的是在特定领域，这里是法律方面的文本分析，缺少数据，但是可以找到很多不相关的网页数据。</p>
<p>这时候迁移学习就会很有用，因为可能实际情况就是这样，我们无法收集太多想要的数据，但是存在很多不直接相关的其他数据。<br><img src="https://img-blog.csdnimg.cn/20210415123031818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其实在现实生活中我们会做迁移学习（有点像类比的思想）。</p>
<p>这里用漫画家的生活对应到研究生的生活。漫画家要画漫画，研究生要跑实验等。<br><img src="https://img-blog.csdnimg.cn/20210415123211214.png" alt="在这里插入图片描述"></p>
<h2 id="1-3-迁移学习的概述"><a href="#1-3-迁移学习的概述" class="headerlink" title="1.3 迁移学习的概述"></a>1.3 迁移学习的概述</h2><p>我们主要把迁移学习分为四大类。<br>在迁移学习中，有一些arget data，就是和你的任务由直接关系的数据；<br>还有很多source data,是和你现在的任务没有直接关系的数据。</p>
<p>根据它们是否有标签，可以分成四类。<br><img src="https://img-blog.csdnimg.cn/2021041512335336.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h3 id="1-3-1-第一类迁移学习"><a href="#1-3-1-第一类迁移学习" class="headerlink" title="1.3.1 第一类迁移学习"></a>1.3.1 第一类迁移学习</h3><p>我们先看下target data和source data都是有标签的情况。</p>
<p>这种情况下我们可以做什么事情呢，一件事情是模型的微调(Fine-tuning)，另一件事情是多任务学习(Multitask Learning)。</p>
<ul>
<li><h2 id="1-模型微调"><a href="#1-模型微调" class="headerlink" title="1) 模型微调"></a>1) 模型微调</h2>设你有一组大量的source data，和一组少量的target data。它们都是有标签的。<br><img src="https://img-blog.csdnimg.cn/20210415123842675.png#pic_center" alt="在这里插入图片描述"><br>你可能听过单样本学习(one-shot learning)：说现在的样本很少，只有几个或一个样本。</li>
</ul>
<p>在语音识别中，我们有大量的source data,我们有几万个人说的不同的句子，并且知道这些句子是什么。target data是某个具体的使用者他说的话，和说的话对应的文字。</p>
<p>因为每个人发音都是不一样的，你拿一大堆人语音的数据训练出来的模型，对某个特定的使用者，可能并不是一定好的。所以我们期望说，假设特定的使用者可以对我们的语音识别系统说5句话，我们知道这5句话对应的文字。</p>
<p>有了这些少量的target data后，就可以拿这些数据让某个特定使用者的语音识别做得更好。这让我想到了”Hey,siri”初次启用时需要说几句话。</p>
<p>这里面的问题是target data数据量很少，所以我们需要特殊的处理方法。一个比较常见的方法叫保守训练(conservative training)。由于篇幅有限这里不将展开，具体详见李宏毅机器学习课件。</p>
<p>接下来我们介绍下多任务学习(Multitask Learning)</p>
<ul>
<li><h2 id="2-多任务学习"><a href="#2-多任务学习" class="headerlink" title="2) 多任务学习"></a>2) 多任务学习</h2>我们现在有多个不同的任务，我们希望机器能同时学会做好这几个不同的任务。</li>
</ul>
<p>比如说你要训练某个人打篮球，同时要训练他唱、跳、Rap。<br><img src="https://img-blog.csdnimg.cn/20210416100235181.gif#pic_center" alt="在这里插入图片描述"></p>
<p>我们希望NN也能做到这件事情。</p>
<p>在这种神经网络的架构设计上可以是像上面这种。这里假设任务A和任务B可以共用同一组输入特征。就是这两个NN，它们前面几层是共用的，但是在某个隐藏层会产生两个分支，一条产生的是任务A的分支，另一条是任务B的。<br><img src="https://img-blog.csdnimg.cn/20210415124649256.png" alt="在这里插入图片描述"><br>那如果这两个任务的输入特征都不能共用呢，我们就可以采用上面的设计，在这两个NN中对不同的输入特征做一些转换，然后丢到共用的网络层中去，再从共用的层中分两个分支出来。<br><img src="https://img-blog.csdnimg.cn/20210415124708437.png" alt="在这里插入图片描述"><br>如果可以选择适当的不同的任务合在一起的话，是可以有帮助的。<br>什么样的任务可能有帮助呢，举例来说，现在在做语音识别的时候，我们不仅让机器学会某国语言的语音识别，我们让机器学会多国语言的。<br><img src="https://img-blog.csdnimg.cn/20210415124809586.png" alt="在这里插入图片描述"><br>此时，多任务学习就会有帮助。<br>这多国语言前面几层是共用的，因为不同的语音声音讯号是一样的(人类的语言都会有一些同样的特征，比如中文里面的嘿和英语里面的hey发音很像)。从这些共用的层出来后分成多个分支，分别做不同国家语言的语音识别。这整个NN可以同时一起训练，这时候学出来的效果比只用一种语言还要好。</p>
<p>这里是文献上的实验的例子，纵轴是错误率，横轴是中文语言识别训练的数据量。<br>从实验结果看到，如果仅让机器学中文的话，就是蓝色的线，它达到红线交点处的错误率需要的中文数据量会超过同时与欧洲语言一起学习的数据量。并且可以看到橙色的曲线是在蓝色曲线的下方，说明效果更加好。<br><img src="https://img-blog.csdnimg.cn/20210415125108449.png" alt="在这里插入图片描述"><br>还有另外一个任务学习方法叫渐进式网络(Progressive Neural Networks)，这里不将展开。</p>
<h3 id="1-3-2-第二类迁移学习"><a href="#1-3-2-第二类迁移学习" class="headerlink" title="1.3.2 第二类迁移学习"></a>1.3.2 第二类迁移学习</h3><p>上面介绍的都是source data和target data有标签的情况，那如果只是source data有标签，target data无标签呢。这种类型也有两种情况，第一种是领域对抗性训练(Domain Adversarial Training)，第二种是零次学习(Zero-shot Learning)。第二种情况是第二部分代码实现的内容。</p>
<ul>
<li><h2 id="1-领域对抗性训练-Domain-Adversarial-Training"><a href="#1-领域对抗性训练-Domain-Adversarial-Training" class="headerlink" title="1)领域对抗性训练(Domain Adversarial Training)"></a>1)领域对抗性训练(Domain Adversarial Training)</h2>这种情况的前提是他们有相同的任务，在概念上你可以把有标签的source data当成训练数据，把无标签的target data当成测试数据，但是这样的效果肯定是很差的，因为它们的分布不同。<br><img src="https://img-blog.csdnimg.cn/20210415140556375.png" alt="在这里插入图片描述"><br>假设今天要做手写数字识别，你有有标签的MNIST的数据，但是你要识别的对象是无标签的来自MNIST-M的数据，在MNIST-M中的数字甚至是彩色的，它的数据样本分布和原来的MNIST分布不一样。<br><img src="https://img-blog.csdnimg.cn/20210415141052765.png" alt="在这里插入图片描述"><br>所以需要特别的处理。Domain-adversarial training就是干这件事的。Domain-adversarial training可以看成GAN的一种。它想要把source data和target data转换到同样的领域上，让它们有同样的分布。<br><img src="https://img-blog.csdnimg.cn/20210415141833915.png" alt="在这里插入图片描述"><br>如果我们没有对数据做任何处理，单纯的拿source data来训练一个分类器，它输入是一个图像，输出是该图形的类别。那今天得到的特征分布可能是下面这样子。<br><img src="https://img-blog.csdnimg.cn/20210415141414902.png" alt="在这里插入图片描述"><br>MNIST的数据它是蓝色的点，确实可以看到它们分成一群一群的，把几群数据的点拿出来看的话，得到的结果可能是左边的样子，能区分出4,0和1。 但是把和MNIST分布不同的MNIST-M手写数字的图片丢到这个分类器中去，这些不一样的图片，它们的特征分布可能像红点一样。可以看到，红点和蓝点根本没有交集。<br>如果今天这个NN无法用同样的特征表示这两种数据，那么就会无法得到好的分类结果。</li>
</ul>
<p>怎么办呢</p>
<p>我们希望在一个NN中，前面几个网络层做的事是特征抽取，如图1所示，也就是说，希望这个特征抽取器能把不同领域的source data和target data都转成同样的特征。<img src="https://img-blog.csdnimg.cn/img_convert/a0fb7213eb449144f30d5dc5639b2885.png"></p>
<blockquote>
<p>图1 Feature Extractor：特征提取器</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20210415142100358.png" alt="在这里插入图片描述"><br>也就是我们希望说，红点和蓝点的分布不是上面这样，而是像下面混合在一起。<br><img src="https://img-blog.csdnimg.cn/20210415142139276.png" alt="在这里插入图片描述"><br>那怎么让我们这个特征抽取器做到这件事情呢。</p>
<p>这里需要引入一个领域的分类器(domain classifier)，如图2所示，就像我们做GAN的时候引入的鉴别器。它也是一个神经网络。<br><img src="https://img-blog.csdnimg.cn/img_convert/a93819091f5dd5f245495f4e342caba4.png"></p>
<blockquote>
<p>图2  Domain Classifier领域的分类器</p>
</blockquote>
<p>Domain-adversarial training可以看成GAN的一种。它想要把source data和target data转换到同样的领域上，让它们有同样的分布。</p>
<p>这个领域分类器的作用是，要侦测出现在特征抽取器输出的特征是属于哪个领域的(来自哪个分布的)。现在特征抽取器要做的事情是尽量骗过这个领域分类器，而后者是尽量防止被骗。</p>
<p>特征抽取器要做的是去除source 领域和target 领域不一样的地方，让提取出来的特征分布是很接近的，可以骗过领域分类器。</p>
<p>但是如果只有这两个神经网络是不够的。因为绿色的特征抽取器可以轻易的骗过红色的分类器，只要它不管输入是什么，只把所有的输出都变成0就可以了。</p>
<p>所以需要引入另外一个东西叫标签预测器(Label predictor)的东西。<br><img src="https://img-blog.csdnimg.cn/img_convert/69eeea1f5f0fb8b61a8b05200982a564.png"></p>
<blockquote>
<p>图3 Label predictor：标签预测器</p>
</blockquote>
<p>现在特征抽取器不仅要骗过分类器，还要让预测器尽量有准确的预测结果。这是一个很大的神经网络，但是这三个不同的部分有不同的目标。</p>
<p>预测器想要正确的分类输入的图片，分类器想要正确分别输入是来自哪个分布。它们都只能看到特征抽取器抽取后的特征。</p>
<p>抽取器一方面希望可以促使预测器做的好，另一方面要防止分类器做的好。</p>
<p>那么要怎么做呢？</p>
<p>一样用梯度下降来训练，红色的分类器部分要调整参数，去让分辨领域的结果越正确越好；蓝色的预测器需要调参数，让标签的预测正确率越高越好；如图4所示梯度反向传播过程。</p>
<p>这两者不一样的地方在于，当分类器要求绿色的抽取器去调整参数以满足以及的目标时，绿色的抽取器会尽量满足它的要求；还当红色的神经网络要求绿色的神经网络调整参数的时候，红色的网络会故意乘以− 1 -1−1，以防止分类器做的好。</p>
<p>最后红色的神经网路会无法做好分类，但是它必须要努力挣扎，它需要从绿色的NN给的不好的特征里面尽量去区分它们的领域。这样才能迫使绿色的NN产生红色的NN无法分辨的特征。难点就在于让红色的NN努力挣扎而不是很快放弃。<br><img src="https://img-blog.csdnimg.cn/20210415142358336.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>图4 Domain Adversarial Training梯度反向传播过程</p>
</blockquote>
<ul>
<li><h2 id="2-零次学习（Zero-shot-Learning）"><a href="#2-零次学习（Zero-shot-Learning）" class="headerlink" title="2)零次学习（Zero-shot Learning）"></a>2)零次学习（Zero-shot Learning）</h2>零次学习(Zero-shot Learning)说的是source data和target data它们的任务都不相同。<br><img src="https://img-blog.csdnimg.cn/20210415142857386.png" alt="在这里插入图片描述"><br>比如source data可能是要做猫和狗的分类；但是target data要做的是做草泥马和羊的分类。<br><img src="https://img-blog.csdnimg.cn/20210415142909487.png" alt="在这里插入图片描述"><br>target data中需要正确找出草泥马，但是source data中都没出现过草泥马，那要怎么做这件事情呢<br>我们先看下语音识别里面是怎么做的，语音识别一直都有训练数据(source data)和测试数据(target data)是不同任务的问题。 很有可能在测试数据中出现的词汇，在训练数据中从来没有出现过。语音识别在处理这个问题的时候，做法是找出比词汇更小的单位。通常语音识别都是拿音位(phoneme，可以理解为音标)做为单位。</li>
</ul>
<p>如果把词汇都转成音位，在识别的时候只去识别音位，然后再把音位转换为词汇的话就可以解决训练数据和测试数据不一样的问题。</p>
<p>其实在图像上的处理方法也很类似，这里不展开。</p>
<h3 id="1-3-3-第三类迁移学习"><a href="#1-3-3-第三类迁移学习" class="headerlink" title="1.3.3 第三类迁移学习"></a>1.3.3 第三类迁移学习</h3><ul>
<li><h2 id="自我学习"><a href="#自我学习" class="headerlink" title="自我学习"></a>自我学习</h2>自我学习(Self-taught learning)其实和半监督学习很像，都是有少量的有标签数据，和非常多的无标签数据。但是与半监督学习有个很大的不同是，有标签数据可能和无标签数据是没有关系的。<h3 id="1-3-4-第四类迁移学习"><a href="#1-3-4-第四类迁移学习" class="headerlink" title="1.3.4 第四类迁移学习"></a>1.3.4 第四类迁移学习</h3></li>
<li><h2 id="自学成簇"><a href="#自学成簇" class="headerlink" title="自学成簇"></a>自学成簇</h2>如果target data和source data都是无标签的话，可以用Self-taught Clustering来做。<br>可以用无标签的source data，可以学出一个较好的特征表示，再用这个较好的特征表示用在聚类上，就可以得到较好的结果。<h1 id="第二部分：领域对抗性训练-Domain-Adversarial-Training-代码实现"><a href="#第二部分：领域对抗性训练-Domain-Adversarial-Training-代码实现" class="headerlink" title="第二部分：领域对抗性训练(Domain Adversarial Training)代码实现"></a>第二部分：领域对抗性训练(Domain Adversarial Training)代码实现</h1><h2 id="2-1-项目描述"><a href="#2-1-项目描述" class="headerlink" title="2.1 项目描述"></a>2.1 项目描述</h2>本作业的任务是迁移学习中的领域对抗性训练(Domain Adversarial Training)。  <blockquote>
<p>也就是左下角的那一块。</p>
</blockquote>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c45384f70bd12de95861b9fdfcc724b0.png"><br>Domain Adaptation是让模型可以在训练时只需要 A dataset label，不需要 B dataset label 的情况下提高 B dataset 的准确率。 （A dataset &amp; task 接近 B dataset &amp; task）也就是给定真实图片 &amp; 标签以及大量的手绘图片，请设计一种方法使得模型可以预测出手绘图片的标签是什么。<br><img src="https://img-blog.csdnimg.cn/img_convert/8c577e960c5a01067c1a90109ef61dd0.png"><br><img src="https://img-blog.csdnimg.cn/img_convert/8f4a8c8d55946784b2c7bb986017130c.png"></p>
<h2 id="2-2-数据集介绍"><a href="#2-2-数据集介绍" class="headerlink" title="2.2 数据集介绍"></a>2.2 数据集介绍</h2><p>这次的任务是源数据: 真实照片，目标数据: 手画涂鸦。<br>我们必须让model看过真实照片以及标签，尝试去预测手画涂鸦的标签为何。<br>资料位于’data/data58171/real_or_drawing.zip’</p>
<ul>
<li>Training : 5000 张真实图片 + label, 32 x 32 RGB</li>
<li>Testing : 100000 张手绘图片，28 x 28 Gray Scale</li>
<li>Label: 总共需要预测 10 个 class。</li>
<li>资料下载下来是以 0 ~ 9 作为label<br>特别注意一点: <strong>这次的源数据和目标数据的图片都是平衡的，你们可以使用这个资料做其他事情。</strong><h3 id="项目要求"><a href="#项目要求" class="headerlink" title="项目要求"></a>项目要求</h3></li>
<li>禁止手动标记label或在网上寻找label</li>
<li>禁止使用pre-trained model<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1769443">项目传送门</a><h1 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h1><h2 id="3-1-数据集查看"><a href="#3-1-数据集查看" class="headerlink" title="3.1 数据集查看"></a>3.1 数据集查看</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入相关库</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
  /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/<strong>init</strong>.py:107: DeprecationWarning: Using or importing the ABCs from ‘collections’ instead of from ‘collections.abc’ is deprecated, and in 3.8 it will stop working<pre><code>from collections import MutableMapping
</code></pre>
  /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from ‘collections’ instead of from ‘collections.abc’ is deprecated, and in 3.8 it will stop working<pre><code>from collections import Iterable, Mapping
</code></pre>
  /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from ‘collections’ instead of from ‘collections.abc’ is deprecated, and in 3.8 it will stop working<pre><code>from collections import Sized
</code></pre>
  2021-04-14 17:30:09,287 - INFO - font search path [‘/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf’, ‘/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm’, ‘/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts’]<br>  2021-04-14 17:30:09,624 - INFO - generated new fontManager<br>展示一下训练集<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">no_axis_show</span>(<span class="params">img, title=<span class="string">&#x27;&#x27;</span>, cmap=<span class="literal">None</span></span>):</span></span><br><span class="line">  <span class="comment"># imshow, 縮放模式為nearest。</span></span><br><span class="line">  fig = plt.imshow(img, interpolation=<span class="string">&#x27;nearest&#x27;</span>, cmap=cmap)</span><br><span class="line">  <span class="comment"># 不要显示axis</span></span><br><span class="line">  fig.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">  fig.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">  plt.title(title)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#标签映射</span></span><br><span class="line">titles = [<span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;bed&#x27;</span>, <span class="string">&#x27;clock&#x27;</span>, <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;television&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;dolphin&#x27;</span>, <span class="string">&#x27;spider&#x27;</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">  plt.subplot(<span class="number">1</span>, <span class="number">10</span>, i+<span class="number">1</span>)</span><br><span class="line">  fig = no_axis_show(plt.imread(<span class="string">f&#x27;work/real_or_drawing/train_data/<span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="number">500</span>*i&#125;</span>.bmp&#x27;</span>), title=titles[i])</span><br><span class="line"><span class="comment">#  work/real_or_drawing/train_data/1/566.bmp</span></span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/20210415143820728.png#pic_center" alt="在这里插入图片描述"><br>展示一下测试集<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">10</span>, i + <span class="number">1</span>)</span><br><span class="line">    fig = no_axis_show(plt.imread(<span class="string">f&#x27;work/real_or_drawing/test_data/0/0000<span class="subst">&#123;i&#125;</span>.bmp&#x27;</span>), title=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/2021041514385115.png#pic_center" alt="在这里插入图片描述"><h2 id="3-2-Special-Domain-Knowledge"><a href="#3-2-Special-Domain-Knowledge" class="headerlink" title="3.2 Special Domain Knowledge"></a>3.2 Special Domain Knowledge</h2><h3 id="预处理source-data"><a href="#预处理source-data" class="headerlink" title="预处理source data"></a>预处理source data</h3>因为大家涂鸦的时候通常只会画轮廓，我们可以根据这点将source data做点边缘侦测处理，让source data更像target data一点。<br>Canny Edge Detection<br>算法这边不赘述，只教大家怎么用。若有兴趣欢迎参考wiki或这里。<br>cv2.Canny使用非常方便，只需要两个参数: low_threshold, high_threshold。<blockquote>
<p>cv2.Canny(image, low_threshold, high_threshold)</p>
</blockquote>
</li>
</ul>
<p>简单来说就是当边缘值超过high_threshold，我们就确定它是edge。如果只有超过low_threshold，那就先判断一下再决定是不是edge。</p>
<p>以下我们直接拿source data做做看。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">titles = [<span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;bed&#x27;</span>, <span class="string">&#x27;clock&#x27;</span>, <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;television&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;dolphin&#x27;</span>, <span class="string">&#x27;spider&#x27;</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line"></span><br><span class="line">original_img = plt.imread(<span class="string">f&#x27;work/real_or_drawing/train_data/0/464.bmp&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">no_axis_show(original_img, title=<span class="string">&#x27;original&#x27;</span>)</span><br><span class="line"></span><br><span class="line">gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line">no_axis_show(gray_img, title=<span class="string">&#x27;gray scale&#x27;</span>, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">canny_50100 = cv2.Canny(gray_img, <span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">no_axis_show(canny_50100, title=<span class="string">&#x27;Canny(50, 100)&#x27;</span>, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">canny_150200 = cv2.Canny(gray_img, <span class="number">150</span>, <span class="number">200</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">no_axis_show(canny_150200, title=<span class="string">&#x27;Canny(150, 200)&#x27;</span>, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">canny_250300 = cv2.Canny(gray_img, <span class="number">250</span>, <span class="number">300</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">no_axis_show(canny_250300, title=<span class="string">&#x27;Canny(250, 300)&#x27;</span>, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210415144213282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="3-4-Data-Process"><a href="#3-4-Data-Process" class="headerlink" title="3.4 Data Process"></a>3.4 Data Process</h2><p>在这里因为train_data的格式已经标注好每种图片，可以直接使用<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/datasets/folder/DatasetFolder_cn.html">paddle.vision.datasets.DatasetFolder</a>。所以只要使用这个API便可以做出一个datasets。在这里要是说明的是用DataFolder读取的时候有两个存放位置，这两个位置分别存放图片和标签。</p>
<p>此外还有数据预处理部分见下面代码：</p>
<h3 id="3-4-1-数据预处理"><a href="#3-4-1-数据预处理" class="headerlink" title="3.4.1 数据预处理"></a>3.4.1 数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.vision.transforms <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> paddle.vision.datasets <span class="keyword">import</span> DatasetFolder,ImageFolder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">source_transform</span>(<span class="params">imge</span>):</span></span><br><span class="line">    <span class="comment"># 转灰色: Canny 不吃 RGB。</span></span><br><span class="line">    img = T.to_grayscale(imge)</span><br><span class="line">    <span class="comment"># cv2 不吃 skimage.Image，因此转成np.array后再做cv2.Canny</span></span><br><span class="line">    img = cv2.Canny(np.array(img), <span class="number">170</span>, <span class="number">300</span>)</span><br><span class="line">    <span class="comment"># 重新np.array 转回 skimage.Image</span></span><br><span class="line">    img = Image.fromarray(np.array(img))</span><br><span class="line">    <span class="comment"># 随机水平翻转 (Augmentation)</span></span><br><span class="line">    RHF= T.RandomHorizontalFlip(<span class="number">0.5</span>)</span><br><span class="line">    img = RHF(img)</span><br><span class="line">    <span class="comment"># 旋转15度内 (Augmentation)，旋转后空的地方补0</span></span><br><span class="line">    RR = T.RandomRotation(<span class="number">15</span>, fill=(<span class="number">0</span>,))</span><br><span class="line">    img = RR(img)</span><br><span class="line">    <span class="comment"># 最后Tensor供model使用。</span></span><br><span class="line">    tensor = T.ToTensor()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tensor(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集预处理</span></span><br><span class="line">target_transform = T.Compose([</span><br><span class="line">    <span class="comment"># 转灰阶:</span></span><br><span class="line">   T.Grayscale(),</span><br><span class="line">    <span class="comment"># 缩放: 因为source data是32x32，我们把target data的28x28放大成32x32。</span></span><br><span class="line">    T.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">    <span class="comment"># 随机水平翻转(Augmentation)</span></span><br><span class="line">    T.RandomHorizontalFlip(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 旋转15度内 (Augmentation)，旋转后空的地方补0</span></span><br><span class="line">    T.RandomRotation(<span class="number">15</span>, fill=(<span class="number">0</span>,)),</span><br><span class="line">    <span class="comment"># 最后Tensor供model使用。</span></span><br><span class="line">    T.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">#下面调用一下数据预处理函数</span></span><br><span class="line">original_img = Image.<span class="built_in">open</span>(<span class="string">f&#x27;work/real_or_drawing/train_data/0/464.bmp&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原来的照片形状：&#x27;</span>,np.array(original_img).shape)</span><br><span class="line">process = source_transform(original_img)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预处理后的照片形状：&#x27;</span>,process .shape)</span><br><span class="line"><span class="built_in">print</span>(process)</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">no_axis_show(process .numpy().squeeze(), title=<span class="string">&#x27;process image&#x27;</span>,cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">no_axis_show(original_img, title=<span class="string">&#x27;origimal image&#x27;</span>, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>原来的照片形状： (32, 32, 3)
预处理后的照片形状： [1, 32, 32]
Tensor(shape=[1, 32, 32], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
       [[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]])
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20210415144533297.png" alt="在这里插入图片描述"></p>
<h3 id="3-4-2-数据加载器定义"><a href="#3-4-2-数据加载器定义" class="headerlink" title="3.4.2 数据加载器定义"></a>3.4.2 数据加载器定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">source_dataset = DatasetFolder(<span class="string">&#x27;work/real_or_drawing/train_data&#x27;</span>, transform=source_transform) <span class="comment"># DatasetFolder 用于读取训练集，读取的时候图片和标签</span></span><br><span class="line">target_dataset = DatasetFolder(<span class="string">&#x27;work/real_or_drawing/test_data&#x27;</span>, transform=target_transform) <span class="comment"># ImageFolder 用于读取测试集，读取的时候只有图片</span></span><br><span class="line"><span class="comment"># 数据加载器定义</span></span><br><span class="line">source_dataloader = paddle.io.DataLoader(source_dataset, batch_size=<span class="number">50</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">target_dataloader = paddle.io.DataLoader(target_dataset, batch_size=<span class="number">50</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = paddle.io.DataLoader(target_dataset, batch_size=<span class="number">100</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># 展示生成并经过预处理的的source_dataset和source_loader</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=============source_dataset=============&#x27;</span>)</span><br><span class="line"><span class="comment">#由于使用了DatasetFolder，训练集这里有图片和标签两个参数image,label</span></span><br><span class="line"><span class="keyword">for</span> image, label <span class="keyword">in</span> source_dataset:      </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;image shape: &#123;&#125;, label: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(image.shape,label))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;训练集数量:&#x27;</span>,<span class="built_in">len</span>(source_dataset))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;图片：&#x27;</span>,image)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;标签：&#x27;</span>,label)</span><br><span class="line">    plt.imshow(image.numpy().squeeze(),cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>=============source_dataset=============
image shape: [1, 32, 32], label: 0
训练集数量: 5000
图片： Tensor(shape=[1, 32, 32], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
       [[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]])
标签： 0
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20210415144635963.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#source_loader的信息    </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=============source_dataloader=============&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> batch_id, (data,label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(source_dataloader):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;一个batch的图片：&#x27;</span>,data.shape)    <span class="comment"># 索引[0]存放图片</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;一个batch的标签个数：&#x27;</span>,label.shape)   <span class="comment">#索引[1]存放标签</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;图片：&#x27;</span>,data[<span class="number">0</span>].shape) </span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="comment"># no_axis_show(x_data.numpy().squeeze(),title=&#x27;process image&#x27;, cmap=&#x27;gray&#x27;)</span></span><br></pre></td></tr></table></figure>
<pre><code>=============source_dataloader=============
一个batch的图片： [50, 1, 32, 32]
一个batch的标签个数： [50]
图片： [1, 32, 32]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展示生成并经过预处理的target_dataset和target_dataloader</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=============target_dataset=============&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> image_,_ <span class="keyword">in</span> target_dataset:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;image shape: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(image_.shape))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;测试集数量:&#x27;</span>,<span class="built_in">len</span>(target_dataset))</span><br><span class="line">    plt.imshow(image_.numpy().squeeze(),cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;图片：&#x27;</span>,image_)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>=============target_dataset=============
image shape: [1, 32, 32]
测试集数量: 100000
图片： Tensor(shape=[1, 32, 32], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
       [[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]])
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20210415144820434.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#target_dataloader的信息    </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=============target_dataloader=============&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> batch_id, (data_1,label_1) <span class="keyword">in</span> <span class="built_in">enumerate</span>(target_dataloader):</span><br><span class="line">    <span class="comment"># print(&#x27;一个batch的图片：&#x27;,data[0].shape)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;一个batch的图片：&#x27;</span>,data_1.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;一张图片的形状：&#x27;</span>,data_1[<span class="number">0</span>].shape) </span><br><span class="line">    <span class="built_in">print</span>(label_1)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>=============target_dataloader=============
一个batch的图片： [50, 1, 32, 32]
一张图片的形状： [1, 32, 32]
Tensor(shape=[50], dtype=int64, place=CUDAPinnedPlace, stop_gradient=True,
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
</code></pre>
<h2 id="3-5-搭建三个模型"><a href="#3-5-搭建三个模型" class="headerlink" title="3.5 搭建三个模型"></a>3.5 搭建三个模型</h2><p>这里的原理参考本文的1.3.2 第二类迁移学习的领域对抗性训练(Domain Adversarial Training)。</p>
<ul>
<li>Feature Extractor: 典型的VGG-like叠法。</li>
<li>Label Predictor ：MLP到尾</li>
<li>Domain Classifier: MLP到尾。</li>
</ul>
<p>特征抽取器不仅要骗过分类器，还要让预测器尽量有准确的预测结果。这是一个很大的神经网络，但是这三个不同的部分有不同的目标。</p>
<p>预测器想要正确的分类输入的图片，分类器想要正确分别输入是来自哪个分布。它们都只能看到特征抽取器抽取后的特征</p>
<p>抽取器一方面希望可以促使预测器做的好，另一方面要防止分类器做的好。</p>
<p>那么要怎么做呢？详见下面的模型训练部分。<br><img src="https://img-blog.csdnimg.cn/img_convert/69eeea1f5f0fb8b61a8b05200982a564.png"></p>
<h3 id="3-5-1-搭建模型"><a href="#3-5-1-搭建模型" class="headerlink" title="3.5.1 搭建模型"></a>3.5.1 搭建模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span>(<span class="params">nn.Layer</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    从图片中抽取特征</span></span><br><span class="line"><span class="string">    input [batch_size ,1,32,32]</span></span><br><span class="line"><span class="string">    output [batch_size ,512]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FeatureExtractor, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv = nn.Sequential(                               </span><br><span class="line">            nn.Conv2D(in_channels=<span class="number">1</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>,  stride=<span class="number">1</span>),  <span class="comment"># [batch_size ,64,32,32] (32-3+2*1)/1 + 1</span></span><br><span class="line">            nn.BatchNorm2D(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2D(kernel_size=<span class="number">2</span>),  <span class="comment"># [batch_size ,64,16,16]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2D(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),  <span class="comment"># [batch_size ,128,16,16]</span></span><br><span class="line">            nn.BatchNorm2D(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2D(<span class="number">2</span>),  <span class="comment"># [batch_size ,128,8,8]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2D(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),  <span class="comment"># [batch_size ,256,8,8]</span></span><br><span class="line">            nn.BatchNorm2D(<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2D(<span class="number">2</span>),  <span class="comment"># [batch_size ,256,4,4]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2D(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),  <span class="comment"># [batch_size ,256,4,4]</span></span><br><span class="line">            nn.BatchNorm2D(<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2D(<span class="number">2</span>),  <span class="comment"># [batch_size ,256,2,2]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2D(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),  <span class="comment"># [batch_size ,512,2,2]</span></span><br><span class="line">            nn.BatchNorm2D(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2D(<span class="number">2</span>),  <span class="comment"># [batch_size ,512,1,1]</span></span><br><span class="line">            nn.Flatten()      <span class="comment"># [batch_size ,512]</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv(x) <span class="comment"># [batch_size ,256]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelPredictor</span>(<span class="params">nn.Layer</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    预测图像是什么动物</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LabelPredictor, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.layer = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">512</span>,<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, h</span>):</span></span><br><span class="line">        c = self.layer(h)</span><br><span class="line">        <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DomainClassifier</span>(<span class="params">nn.Layer</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;预测时手绘还是真实图片&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DomainClassifier, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.layer = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.BatchNorm1D(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.BatchNorm1D(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.BatchNorm1D(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.BatchNorm1D(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, h</span>):</span></span><br><span class="line">        y = self.layer(h)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h3 id="3-5-2-模型配置"><a href="#3-5-2-模型配置" class="headerlink" title="3.5.2 模型配置"></a>3.5.2 模型配置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.optimizer <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 模型实例化</span></span><br><span class="line">feature_extractor = FeatureExtractor()</span><br><span class="line">label_predictor = LabelPredictor()</span><br><span class="line">domain_classifier = DomainClassifier()</span><br><span class="line">class_criterion = nn.CrossEntropyLoss()</span><br><span class="line">domain_criterion = nn.BCEWithLogitsLoss()</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer_F = optim.Adam(learning_rate=<span class="number">0.0001</span>, parameters=feature_extractor.parameters())</span><br><span class="line">optimizer_C = optim.Adam(learning_rate=<span class="number">0.0001</span>, parameters=label_predictor.parameters())</span><br><span class="line">optimizer_D = optim.Adam(learning_rate=<span class="number">0.0001</span>, parameters=domain_classifier.parameters())</span><br></pre></td></tr></table></figure>
<h3 id="3-5-3-开始训练"><a href="#3-5-3-开始训练" class="headerlink" title="3.5.3 开始训练"></a>3.5.3 开始训练</h3><p>用梯度下降来训练，红色的分类器部分要调整参数，去让分辨领域的结果越正确越好；蓝色的预测器需要调参数，让标签的预测正确率越高越好；</p>
<p>这两者不一样的地方在于，当分类器要求绿色的抽取器去调整参数以满足以及的目标时，绿色的抽取器会尽量满足它的要求；还当红色的神经网络要求绿色的神经网络调整参数的时候，红色的网络会故意乘以-1，以防止分类器做的好。</p>
<p>最后红色的神经网路会无法做好分类，但是它必须要努力挣扎，它需要从绿色的NN给的不好的特征里面尽量去区分它们的领域。这样才能迫使绿色的NN产生红色的NN无法分辨的特征。难点就在于让红色的NN努力挣扎而不是很快放弃。<br><img src="https://img-blog.csdnimg.cn/img_convert/370c88f7f57d4b7b564a36cab7aaba3b.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练函数</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch</span>(<span class="params">source_dataloader, target_dataloader, lamb</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">      Args:</span></span><br><span class="line"><span class="string">        source_dataloader: source data的dataloader</span></span><br><span class="line"><span class="string">        target_dataloader: target data的dataloader</span></span><br><span class="line"><span class="string">        lamb: 调控adversarial的loss系数。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    running_D_loss, running_F_loss = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">    total_hit, total_num = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, ((source_data, source_label), (target_data,_)) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(source_dataloader, target_dataloader)):</span><br><span class="line">        mixed_data = paddle.concat([source_data, target_data], axis=<span class="number">0</span>)</span><br><span class="line">        domain_label = paddle.zeros([source_data.shape[<span class="number">0</span>] + target_data.shape[<span class="number">0</span>], <span class="number">1</span>]).cuda()</span><br><span class="line">        <span class="comment"># 设定source data的label为1</span></span><br><span class="line">        domain_label[:source_data.shape[<span class="number">0</span>]] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1 : 训练Domain Classifier</span></span><br><span class="line">        feature = feature_extractor(mixed_data)</span><br><span class="line">        <span class="comment"># 因为我们在Step 1不需要训练Feature Extractor，所以把feature detach</span></span><br><span class="line">        <span class="comment">#这样可以把特征抽取过程的函数从当前计算图分离，避免loss backprop传递过去。</span></span><br><span class="line">        domain_logits = domain_classifier(feature.detach())</span><br><span class="line">        loss = domain_criterion(domain_logits, domain_label)</span><br><span class="line">        running_D_loss += loss.numpy().tolist()[<span class="number">0</span>]</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer_D.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2 : 训练Feature Extractor和Domain Classifier</span></span><br><span class="line">        class_logits = label_predictor(feature[:source_data.shape[<span class="number">0</span>]])</span><br><span class="line">        domain_logits = domain_classifier(feature)</span><br><span class="line">        <span class="comment"># loss为原本的class CE - lamb * domain BCE，相減的原因是我们希望特征能够使得domain_classifier分不出来输入的图片属于哪个领域</span></span><br><span class="line">        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)</span><br><span class="line">        running_F_loss += loss.numpy().tolist()[<span class="number">0</span>]</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer_F.step()</span><br><span class="line">        optimizer_C.step()</span><br><span class="line">        <span class="comment">#训练了一轮，清空所有梯度信息</span></span><br><span class="line">        optimizer_D.clear_grad()</span><br><span class="line">        optimizer_F.clear_grad()</span><br><span class="line">        optimizer_C.clear_grad()</span><br><span class="line">        <span class="comment"># return class_logits,source_label  #测试</span></span><br><span class="line">        bool_eq = paddle.argmax(class_logits, axis=<span class="number">1</span>) == source_label.squeeze()</span><br><span class="line">        total_hit += np.<span class="built_in">sum</span>(bool_eq.numpy()!=<span class="number">0</span>)</span><br><span class="line">        total_num += source_data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="built_in">print</span>(i, end=<span class="string">&#x27;\r&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> running_D_loss / (i+<span class="number">1</span>), running_F_loss / (i+<span class="number">1</span>), total_hit / total_num</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练125 epochs</span></span><br><span class="line">train_D_loss_history,train_F_loss_history,train_acc_history = [], [], []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">125</span>):</span><br><span class="line">    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb=<span class="number">0.1</span>)</span><br><span class="line">    train_D_loss_history.append(train_D_loss)</span><br><span class="line">    train_F_loss_history.append(train_F_loss)</span><br><span class="line">    train_acc_history.append(train_acc)  </span><br><span class="line">    epoch = epoch + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        paddle.save(feature_extractor.state_dict(), <span class="string">&quot;ckp/&#123;&#125;ckp_feature_extractor.pdparams&quot;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(epoch)))</span><br><span class="line">        paddle.save(label_predictor.state_dict(), <span class="string">&quot;ckp/&#123;&#125;ckp_label_predictor.pdparams&quot;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(epoch)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch &#123;:&gt;3d&#125;: train D loss: &#123;:6.4f&#125;, train F loss: &#123;:6.4f&#125;, acc &#123;:6.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_D_loss, train_F_loss, train_acc))                                                                            </span><br></pre></td></tr></table></figure>
<pre><code>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:648: UserWarning: When training, we now always track global mean and variance.
  &quot;When training, we now always track global mean and variance.&quot;)
5
epoch   1: train D loss: 0.0202, train F loss: 0.0416, acc 0.9844
epoch   2: train D loss: 0.0291, train F loss: 0.0400, acc 0.9824
epoch   3: train D loss: 0.0308, train F loss: 0.0378, acc 0.9872
epoch   4: train D loss: 0.0351, train F loss: 0.0576, acc 0.9792
epoch   5: train D loss: 0.0348, train F loss: 0.0456, acc 0.9830
epoch   6: train D loss: 0.0395, train F loss: 0.0360, acc 0.9860
epoch   7: train D loss: 0.0353, train F loss: 0.0508, acc 0.9822
epoch   8: train D loss: 0.0390, train F loss: 0.0348, acc 0.9874
epoch   9: train D loss: 0.0413, train F loss: 0.0503, acc 0.9808
epoch  10: train D loss: 0.0440, train F loss: 0.0480, acc 0.9796
epoch  11: train D loss: 0.0413, train F loss: 0.0428, acc 0.9834
epoch  12: train D loss: 0.0422, train F loss: 0.0402, acc 0.9842
epoch  13: train D loss: 0.0512, train F loss: 0.0506, acc 0.9782
epoch  14: train D loss: 0.0519, train F loss: 0.0549, acc 0.9814
epoch  15: train D loss: 0.0446, train F loss: 0.0309, acc 0.9878
epoch  16: train D loss: 0.0485, train F loss: 0.0395, acc 0.9858
epoch  17: train D loss: 0.0531, train F loss: 0.0445, acc 0.9826
epoch  18: train D loss: 0.0507, train F loss: 0.0370, acc 0.9864
epoch  19: train D loss: 0.0525, train F loss: 0.0516, acc 0.9812
epoch  20: train D loss: 0.0546, train F loss: 0.0422, acc 0.9832
epoch  21: train D loss: 0.0522, train F loss: 0.0407, acc 0.9856
epoch  22: train D loss: 0.0541, train F loss: 0.0248, acc 0.9884
epoch  23: train D loss: 0.0537, train F loss: 0.0352, acc 0.9872
epoch  24: train D loss: 0.0517, train F loss: 0.0291, acc 0.9884
epoch  25: train D loss: 0.0611, train F loss: 0.0304, acc 0.9866
epoch  26: train D loss: 0.0590, train F loss: 0.0407, acc 0.9840
epoch  27: train D loss: 0.0588, train F loss: 0.0312, acc 0.9886
epoch  28: train D loss: 0.0569, train F loss: 0.0347, acc 0.9852
epoch  29: train D loss: 0.0586, train F loss: 0.0501, acc 0.9810
epoch  30: train D loss: 0.0563, train F loss: 0.0530, acc 0.9796
epoch  31: train D loss: 0.0699, train F loss: 0.0683, acc 0.9734
epoch  32: train D loss: 0.0577, train F loss: 0.0367, acc 0.9862
epoch  33: train D loss: 0.0546, train F loss: 0.0385, acc 0.9860
epoch  34: train D loss: 0.0669, train F loss: 0.0304, acc 0.9868
epoch  35: train D loss: 0.0629, train F loss: 0.0350, acc 0.9846
epoch  36: train D loss: 0.0573, train F loss: 0.0194, acc 0.9918
epoch  37: train D loss: 0.0660, train F loss: 0.0222, acc 0.9886
epoch  38: train D loss: 0.0702, train F loss: 0.0388, acc 0.9852
epoch  39: train D loss: 0.0710, train F loss: 0.0333, acc 0.9868
epoch  40: train D loss: 0.0724, train F loss: 0.0372, acc 0.9828
epoch  41: train D loss: 0.0731, train F loss: 0.0312, acc 0.9856
epoch  42: train D loss: 0.0744, train F loss: 0.0263, acc 0.9890
epoch  43: train D loss: 0.0788, train F loss: 0.0238, acc 0.9876
epoch  44: train D loss: 0.0806, train F loss: 0.0312, acc 0.9862
epoch  45: train D loss: 0.0726, train F loss: 0.0442, acc 0.9808
epoch  46: train D loss: 0.0763, train F loss: 0.0461, acc 0.9814
epoch  47: train D loss: 0.0765, train F loss: 0.0501, acc 0.9818
epoch  48: train D loss: 0.0770, train F loss: 0.0327, acc 0.9884
epoch  49: train D loss: 0.0789, train F loss: 0.0294, acc 0.9874
epoch  50: train D loss: 0.0841, train F loss: 0.0306, acc 0.9860
epoch  51: train D loss: 0.0807, train F loss: 0.0439, acc 0.9810
epoch  52: train D loss: 0.0742, train F loss: 0.0327, acc 0.9872
epoch  53: train D loss: 0.0797, train F loss: 0.0293, acc 0.9870
epoch  54: train D loss: 0.0826, train F loss: 0.0342, acc 0.9848
epoch  55: train D loss: 0.0840, train F loss: 0.0353, acc 0.9846
epoch  56: train D loss: 0.0810, train F loss: 0.0187, acc 0.9898
epoch  57: train D loss: 0.0846, train F loss: 0.0278, acc 0.9878
epoch  58: train D loss: 0.0878, train F loss: 0.0430, acc 0.9820
epoch  59: train D loss: 0.0933, train F loss: 0.0413, acc 0.9828
epoch  60: train D loss: 0.0856, train F loss: 0.0380, acc 0.9864
epoch  61: train D loss: 0.0883, train F loss: 0.0312, acc 0.9856
epoch  62: train D loss: 0.0851, train F loss: 0.0281, acc 0.9888
epoch  63: train D loss: 0.0929, train F loss: 0.0244, acc 0.9886
epoch  64: train D loss: 0.0968, train F loss: 0.0327, acc 0.9848
epoch  65: train D loss: 0.0973, train F loss: 0.0300, acc 0.9866
epoch  66: train D loss: 0.1008, train F loss: 0.0298, acc 0.9860
epoch  67: train D loss: 0.0987, train F loss: 0.0480, acc 0.9790
epoch  68: train D loss: 0.1049, train F loss: 0.0304, acc 0.9856
epoch  69: train D loss: 0.1018, train F loss: 0.0231, acc 0.9870
epoch  70: train D loss: 0.0993, train F loss: 0.0237, acc 0.9874
epoch  71: train D loss: 0.1073, train F loss: 0.0213, acc 0.9896
epoch  72: train D loss: 0.1006, train F loss: 0.0291, acc 0.9874
epoch  73: train D loss: 0.1113, train F loss: 0.0322, acc 0.9864
epoch  74: train D loss: 0.1169, train F loss: 0.0280, acc 0.9864
epoch  75: train D loss: 0.0981, train F loss: 0.0250, acc 0.9866
epoch  76: train D loss: 0.1152, train F loss: 0.0200, acc 0.9894
epoch  77: train D loss: 0.1056, train F loss: 0.0209, acc 0.9884
epoch  78: train D loss: 0.1171, train F loss: 0.0323, acc 0.9834
epoch  79: train D loss: 0.1179, train F loss: 0.0358, acc 0.9834
epoch  80: train D loss: 0.1054, train F loss: 0.0220, acc 0.9884
epoch  81: train D loss: 0.1150, train F loss: 0.0454, acc 0.9808
epoch  82: train D loss: 0.1175, train F loss: 0.0211, acc 0.9900
epoch  83: train D loss: 0.1161, train F loss: 0.0178, acc 0.9898
epoch  84: train D loss: 0.1174, train F loss: 0.0285, acc 0.9870
epoch  85: train D loss: 0.1233, train F loss: 0.0360, acc 0.9836
epoch  86: train D loss: 0.1247, train F loss: 0.0277, acc 0.9870
epoch  87: train D loss: 0.1178, train F loss: 0.0126, acc 0.9914
epoch  88: train D loss: 0.1292, train F loss: 0.0260, acc 0.9860
epoch  89: train D loss: 0.1216, train F loss: 0.0266, acc 0.9858
epoch  90: train D loss: 0.1400, train F loss: 0.0245, acc 0.9872
epoch  91: train D loss: 0.1286, train F loss: 0.0178, acc 0.9876
epoch  92: train D loss: 0.1263, train F loss: 0.0142, acc 0.9914
epoch  93: train D loss: 0.1287, train F loss: 0.0249, acc 0.9874
epoch  94: train D loss: 0.1305, train F loss: 0.0230, acc 0.9868
epoch  95: train D loss: 0.1218, train F loss: 0.0244, acc 0.9882
epoch  96: train D loss: 0.1289, train F loss: 0.0261, acc 0.9872
epoch  97: train D loss: 0.1279, train F loss: 0.0220, acc 0.9878
epoch  98: train D loss: 0.1296, train F loss: 0.0240, acc 0.9880
epoch  99: train D loss: 0.1254, train F loss: 0.0158, acc 0.9906
epoch 100: train D loss: 0.1340, train F loss: 0.0096, acc 0.9928
epoch 101: train D loss: 0.1321, train F loss: 0.0208, acc 0.9876
epoch 102: train D loss: 0.1388, train F loss: 0.0338, acc 0.9824
epoch 103: train D loss: 0.1355, train F loss: 0.0224, acc 0.9874
epoch 104: train D loss: 0.1366, train F loss: 0.0405, acc 0.9806
epoch 105: train D loss: 0.1386, train F loss: 0.0367, acc 0.9838
epoch 106: train D loss: 0.1402, train F loss: 0.0294, acc 0.9872
epoch 107: train D loss: 0.1353, train F loss: 0.0310, acc 0.9850
epoch 108: train D loss: 0.1380, train F loss: 0.0107, acc 0.9918
epoch 109: train D loss: 0.1475, train F loss: 0.0178, acc 0.9892
epoch 110: train D loss: 0.1376, train F loss: 0.0189, acc 0.9892
epoch 111: train D loss: 0.1350, train F loss: 0.0119, acc 0.9908
epoch 112: train D loss: 0.1454, train F loss: 0.0132, acc 0.9902
epoch 113: train D loss: 0.1463, train F loss: 0.0373, acc 0.9818
epoch 114: train D loss: 0.1418, train F loss: 0.0376, acc 0.9802
epoch 115: train D loss: 0.1501, train F loss: 0.0323, acc 0.9834
epoch 116: train D loss: 0.1446, train F loss: 0.0132, acc 0.9902
epoch 117: train D loss: 0.1367, train F loss: 0.0181, acc 0.9896
epoch 118: train D loss: 0.1407, train F loss: 0.0171, acc 0.9908
epoch 119: train D loss: 0.1416, train F loss: 0.0169, acc 0.9890
epoch 120: train D loss: 0.1469, train F loss: 0.0152, acc 0.9914
epoch 121: train D loss: 0.1444, train F loss: 0.0141, acc 0.9906
epoch 122: train D loss: 0.1522, train F loss: 0.0237, acc 0.9854
epoch 123: train D loss: 0.1450, train F loss: 0.0274, acc 0.9856
epoch 124: train D loss: 0.1530, train F loss: 0.0134, acc 0.9900
epoch 125: train D loss: 0.1607, train F loss: 0.0277, acc 0.9848
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">paddle.save(feature_extractor.state_dict(), <span class="string">&quot;model/feature_extractor_final.pdparams&quot;</span>)</span><br><span class="line">paddle.save(label_predictor.state_dict(), <span class="string">&quot;model/label_predictor_final.pdparams&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-5-4-可视化训练过程"><a href="#3-5-4-可视化训练过程" class="headerlink" title="3.5.4 可视化训练过程"></a>3.5.4 可视化训练过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分开绘制三条曲线</span></span><br><span class="line">epochs = <span class="built_in">range</span>(epoch)</span><br><span class="line"><span class="comment"># 模型训练可视化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_process</span>(<span class="params">title,color,iters,data,label</span>):</span></span><br><span class="line">    plt.title(title, fontsize=<span class="number">20</span>)  <span class="comment"># 标题</span></span><br><span class="line">    plt.xlabel(<span class="string">&quot;epochs&quot;</span>, fontsize=<span class="number">15</span>)  <span class="comment"># x轴</span></span><br><span class="line">    plt.ylabel(label, fontsize=<span class="number">15</span>)  <span class="comment"># y轴</span></span><br><span class="line">    plt.plot(iters, data,color=color,label=label)   <span class="comment"># 画图</span></span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.savefig(<span class="string">&#x27;&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(title))</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># Domain Classifier train loss</span></span><br><span class="line">draw_process(<span class="string">&quot;train D loss&quot;</span>,<span class="string">&quot;green&quot;</span>,epochs,train_D_loss_history,<span class="string">&quot;loss&quot;</span>) </span><br><span class="line"><span class="comment"># Feature Extrator train loss</span></span><br><span class="line">draw_process(<span class="string">&quot;train F loss&quot;</span>,<span class="string">&quot;green&quot;</span>,epochs,train_F_loss_history,<span class="string">&quot;loss&quot;</span>) </span><br><span class="line"><span class="comment"># Label Predictor的train accuracy</span></span><br><span class="line">draw_process(<span class="string">&quot;train acc&quot;</span>,<span class="string">&quot;red&quot;</span>,epochs,train_acc_history,<span class="string">&quot;accuracy&quot;</span>) </span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210415145842614.jpg?ype_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415145854814.jpg?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415145906343.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="4-模型预测"><a href="#4-模型预测" class="headerlink" title="4 模型预测"></a>4 模型预测</h1><p>在测试集上执行预测</p>
<h2 id="4-1-预测测试集结果"><a href="#4-1-预测测试集结果" class="headerlink" title="4.1 预测测试集结果"></a>4.1 预测测试集结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">result = []</span><br><span class="line">label_predictor.<span class="built_in">eval</span>()</span><br><span class="line">feature_extractor.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> i, (test_data, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_dataloader):</span><br><span class="line">    test_data = test_data.cuda()</span><br><span class="line"></span><br><span class="line">    class_logits = label_predictor(feature_extractor(test_data))</span><br><span class="line"></span><br><span class="line">    x = paddle.argmax(class_logits, axis=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">    result.append(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">result = np.concatenate(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate your submission</span></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;id&#x27;</span>: np.arange(<span class="number">0</span>,<span class="built_in">len</span>(result)), <span class="string">&#x27;label&#x27;</span>: result&#125;)</span><br><span class="line">df.to_csv(<span class="string">&#x27;DaNN_submission.csv&#x27;</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计预测的标签数量，10种图片的预测数量如下：</span></span><br><span class="line"><span class="built_in">print</span>(df.iloc[:,<span class="number">1</span>].value_counts())</span><br></pre></td></tr></table></figure>
<pre><code>5    26514
3    20621
4    10328
7     9979
8     9213
1     9159
6     4518
9     4365
0     3781
2     1522
Name: label, dtype: int64
</code></pre>
<h2 id="4-2-展示预测结果"><a href="#4-2-展示预测结果" class="headerlink" title="4.2 展示预测结果"></a>4.2 展示预测结果</h2><p>展示前一百幅的结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">labels = <span class="built_in">iter</span>(df[<span class="string">&#x27;label&#x27;</span>][<span class="number">0</span>:<span class="number">100</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_names</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">yield</span> <span class="string">&#x27;work/real_or_drawing/test_data/0/&#123;:05&#125;.bmp&#x27;</span>.<span class="built_in">format</span>(i)       </span><br><span class="line">names = <span class="built_in">iter</span>(f_names())</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">10</span>, i + <span class="number">1</span>)</span><br><span class="line">        name = <span class="built_in">next</span>(names)</span><br><span class="line">        label = <span class="built_in">next</span>(labels)</span><br><span class="line">        fig = no_axis_show(plt.imread(name),title=titles[label])</span><br><span class="line"><span class="keyword">yield</span> <span class="string">&#x27;work/real_or_drawing/test_data/0/&#123;:05&#125;.bmp&#x27;</span>.<span class="built_in">format</span>(i)        </span><br><span class="line">names = <span class="built_in">iter</span>(f_names())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">10</span>, i + <span class="number">1</span>)</span><br><span class="line">        name = <span class="built_in">next</span>(names)</span><br><span class="line">        label = <span class="built_in">next</span>(labels)</span><br><span class="line">        fig = no_axis_show(plt.imread(name),title=titles[label])</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210415150045964.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415150045975.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415150045971.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415150045967.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415150045956.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415150045956.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415150045968.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415150045959.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415150045951.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210415150045959.png" alt="在这里插入图片描述"></p>
<h1 id="5-总结分析"><a href="#5-总结分析" class="headerlink" title="5 总结分析"></a>5 总结分析</h1><p>本次项目共进行了三次训练：第一次训练200个epochs,第二次训练125个epoch，第三次250个epoch。<br>可以通过以下的曲线对比，模型的训练可视化如下，可以发现：</p>
<ul>
<li>1） 三次训练中特征抽取器(Feature Extractor)的train F loss曲线都呈现下降趋势。</li>
<li>2） 而epoch=125,和epoch=200时，领域的分类器(Domain Classifier)的train D loss曲线呈现增大的趋势，可能原因是训练不稳定；epoch=250,领域的分类器(Domain Classifier)的train D loss曲线逐渐收敛。</li>
<li>3）三次的训练，标签预测器(Label Lredictor)的acc曲线在上升，最终acc都在0.98左右。</li>
</ul>
<p>特征抽取器就是不断抽取一些领域分类器不一样的特征为了能骗过它。并且他们这样相生相克就是为了模型能有很好的预测能力，这在标签预测器的acc曲线充分地表现了出来。因此，这就是迁移学习——Domain-adversarial training的根本所在！（Domain-adversarial training可以看成GAN的一种。它想要把source data和target data转换到同样的领域上，让它们有同样的分布。）</p>
<blockquote>
<ul>
<li>epoch=125<br>训练过程不稳定</li>
</ul>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/img_convert/428bdfb77336911ac1f5df797d4e4e22.png"><br><img src="https://img-blog.csdnimg.cn/img_convert/4a378a5f4dbe078ec6161382c5d6841c.png"><br><img src="https://img-blog.csdnimg.cn/img_convert/24f7bf78e34fd7b13b401e3db9be523e.png"></p>
<blockquote>
<ul>
<li>epoch=200</li>
</ul>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/img_convert/ec20633f7d6764a70a9ebb672cb9fca1.png"><br><img src="https://img-blog.csdnimg.cn/img_convert/1293fe5e2ec18495617bef612711df9d.png"><br><img src="https://img-blog.csdnimg.cn/img_convert/abbc156df82dbd3d44fdf5f2befd49f1.png"></p>
<blockquote>
<ul>
<li>epoch=250</li>
</ul>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20210416095102961.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210416095113511.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210416095123218.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>模型的前100张测试集结果对比：<br>就前100张预测图片来看，三种预测结果差别还挺大的，因为没有标签，无法得知预测结果好坏。</p>
<blockquote>
<p>epoch=125:</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2a96b01eda15d1f79ac63fec6d2faf9b.png"><br><img src="https://img-blog.csdnimg.cn/img_convert/305bd558dfbbb20033ac809253990c80.png"></p>
<blockquote>
<p>epoch=200<br><img src="https://img-blog.csdnimg.cn/img_convert/f438de59acd27a79c2da6e6f1b0f5562.png"><br><img src="https://img-blog.csdnimg.cn/img_convert/942116228473e0d6b4e03edfc87431b8.png"><br>epoch=250</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20210416095346634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2021041609540751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTI3MjE3Mg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="6-参考文献-amp-文章-amp-代码"><a href="#6-参考文献-amp-文章-amp-代码" class="headerlink" title="6 参考文献&amp;文章&amp;代码"></a>6 参考文献&amp;文章&amp;代码</h1><p>[1] <a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/education/group/info/1978">李宏毅机器学习</a><br>[2] <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44673043/article/details/114858094">https://blog.csdn.net/weixin_44673043/article/details/114858094</a><br>[3] <a target="_blank" rel="noopener" href="https://helloai.blog.csdn.net/article/details/104484924">https://helloai.blog.csdn.net/article/details/104484924</a><br>[4]<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/leeml-notes/#/chapter30/chapter30">https://datawhalechina.github.io/leeml-notes/#/chapter30/chapter30</a></p>
<h1 id="作者介绍"><a href="#作者介绍" class="headerlink" title="作者介绍"></a>作者介绍</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/personalcenter/thirdview/616597">百度AI Studio个人主页</a>, 我在AI Studio上获得白银等级，点亮2个徽章，来互关呀~<br>CSDN:<a target="_blank" rel="noopener" href="https://i.csdn.net/#/user-center/profile?spm=1011.2124.3001.5111">https://i.csdn.net/#/user-center/profile?spm=1011.2124.3001.5111</a><br>交流qq:3207820044</p>
</blockquote>

    </div>

    
    
    
     
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>healerson
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2021/04/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0(Transfer%20Learning)%E6%A6%82%E8%BF%B0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/" title="">http://example.com/2021/04/22/李宏毅机器学习——迁移学习(Transfer Learning)概述及代码实现/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/03/30/json/" rel="prev" title="JSON 库开发">
      <i class="fa fa-chevron-left"></i> JSON 库开发
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EPaddlePaddle%E7%9A%84%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">基于PaddlePaddle的李宏毅机器学习——迁移学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AC%E9%A1%B9%E7%9B%AE%E6%98%AF%E5%9C%A8%E9%A3%9E%E6%A1%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E9%99%A2%E6%8F%90%E4%BE%9B%E7%9A%84%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E8%AE%AD%E8%90%A5%E8%AF%BE%E7%A8%8B%E3%80%82"><span class="nav-number">1.0.1.</span> <span class="nav-text">本项目是在飞桨深度学习学院提供的李宏毅-机器学习特训营课程。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Abstract"><span class="nav-number">2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D"><span class="nav-number">3.</span> <span class="nav-text">第一部分：迁移学习介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%EF%BC%9ATransfer-Learning"><span class="nav-number">3.1.</span> <span class="nav-text">1 迁移学习：Transfer Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AF%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%91%A2%EF%BC%9F"><span class="nav-number">3.2.</span> <span class="nav-text">1.1 什么是迁移学习呢？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.3.</span> <span class="nav-text">1.2 为什么用迁移学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A6%82%E8%BF%B0"><span class="nav-number">3.4.</span> <span class="nav-text">1.3 迁移学习的概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-%E7%AC%AC%E4%B8%80%E7%B1%BB%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.4.1.</span> <span class="nav-text">1.3.1 第一类迁移学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83"><span class="nav-number">3.5.</span> <span class="nav-text">1) 模型微调</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.6.</span> <span class="nav-text">2) 多任务学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-%E7%AC%AC%E4%BA%8C%E7%B1%BB%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.6.1.</span> <span class="nav-text">1.3.2 第二类迁移学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E9%A2%86%E5%9F%9F%E5%AF%B9%E6%8A%97%E6%80%A7%E8%AE%AD%E7%BB%83-Domain-Adversarial-Training"><span class="nav-number">3.7.</span> <span class="nav-text">1)领域对抗性训练(Domain Adversarial Training)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%9B%B6%E6%AC%A1%E5%AD%A6%E4%B9%A0%EF%BC%88Zero-shot-Learning%EF%BC%89"><span class="nav-number">3.8.</span> <span class="nav-text">2)零次学习（Zero-shot Learning）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-%E7%AC%AC%E4%B8%89%E7%B1%BB%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.8.1.</span> <span class="nav-text">1.3.3 第三类迁移学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.9.</span> <span class="nav-text">自我学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-%E7%AC%AC%E5%9B%9B%E7%B1%BB%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.9.1.</span> <span class="nav-text">1.3.4 第四类迁移学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%AD%A6%E6%88%90%E7%B0%87"><span class="nav-number">3.10.</span> <span class="nav-text">自学成簇</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E9%A2%86%E5%9F%9F%E5%AF%B9%E6%8A%97%E6%80%A7%E8%AE%AD%E7%BB%83-Domain-Adversarial-Training-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.</span> <span class="nav-text">第二部分：领域对抗性训练(Domain Adversarial Training)代码实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E9%A1%B9%E7%9B%AE%E6%8F%8F%E8%BF%B0"><span class="nav-number">4.1.</span> <span class="nav-text">2.1 项目描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="nav-number">4.2.</span> <span class="nav-text">2.2 数据集介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E8%A6%81%E6%B1%82"><span class="nav-number">4.2.1.</span> <span class="nav-text">项目要求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">4.2.2.</span> <span class="nav-text">数据准备</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">3 代码实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9F%A5%E7%9C%8B"><span class="nav-number">5.1.</span> <span class="nav-text">3.1 数据集查看</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-Special-Domain-Knowledge"><span class="nav-number">5.2.</span> <span class="nav-text">3.2 Special Domain Knowledge</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86source-data"><span class="nav-number">5.2.1.</span> <span class="nav-text">预处理source data</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-Data-Process"><span class="nav-number">5.3.</span> <span class="nav-text">3.4 Data Process</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">5.3.1.</span> <span class="nav-text">3.4.1 数据预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-2-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8%E5%AE%9A%E4%B9%89"><span class="nav-number">5.3.2.</span> <span class="nav-text">3.4.2 数据加载器定义</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-%E6%90%AD%E5%BB%BA%E4%B8%89%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.4.</span> <span class="nav-text">3.5 搭建三个模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-1-%E6%90%AD%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.4.1.</span> <span class="nav-text">3.5.1 搭建模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-2-%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE"><span class="nav-number">5.4.2.</span> <span class="nav-text">3.5.2 模型配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-3-%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">5.4.3.</span> <span class="nav-text">3.5.3 开始训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-4-%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">5.4.4.</span> <span class="nav-text">3.5.4 可视化训练过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="nav-number">6.</span> <span class="nav-text">4 模型预测</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-%E9%A2%84%E6%B5%8B%E6%B5%8B%E8%AF%95%E9%9B%86%E7%BB%93%E6%9E%9C"><span class="nav-number">6.1.</span> <span class="nav-text">4.1 预测测试集结果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-%E5%B1%95%E7%A4%BA%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-number">6.2.</span> <span class="nav-text">4.2 展示预测结果</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-%E6%80%BB%E7%BB%93%E5%88%86%E6%9E%90"><span class="nav-number">7.</span> <span class="nav-text">5 总结分析</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-amp-%E6%96%87%E7%AB%A0-amp-%E4%BB%A3%E7%A0%81"><span class="nav-number">8.</span> <span class="nav-text">6 参考文献&amp;文章&amp;代码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%9C%E8%80%85%E4%BB%8B%E7%BB%8D"><span class="nav-number">9.</span> <span class="nav-text">作者介绍</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="healerson"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">healerson</p>
  <div class="site-description" itemprop="description">choosing is more important</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/healerson" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;healerson" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/1551908886@qq.com" title="E-Mail → 1551908886@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/" title="Twitter → https:&#x2F;&#x2F;twitter.com" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://youtube.com/" title="YouTube → https:&#x2F;&#x2F;youtube.com" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/" title="Instagram → https:&#x2F;&#x2F;instagram.com" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://https//blog.csdn.net/Later_001" title="CSDN → https:&#x2F;&#x2F;https:&#x2F;&#x2F;blog.csdn.net&#x2F;Later_001" rel="noopener" target="_blank"><i class="fab fa-codiepie fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/mai-nv-hai-de-xiao-huo-chai-35-19" title="zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;mai-nv-hai-de-xiao-huo-chai-35-19" rel="noopener" target="_blank"><i class="fab fa-gratipay fa-fw"></i>zhihu</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://baidu.com/" title="https:&#x2F;&#x2F;baidu.com" rel="noopener" target="_blank">百度</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://fishc.com.cn/" title="https:&#x2F;&#x2F;fishc.com.cn" rel="noopener" target="_blank">鱼c</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.csdn.net/" title="https:&#x2F;&#x2F;www.csdn.net" rel="noopener" target="_blank">CSDN</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021-03 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">healerson</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共14k字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  















  

  

</body>
</html>
